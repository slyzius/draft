using System;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace WindowsFormsApplication1
{
    /// <summary>
    /// Naive Byesain classifier. Category prior is joint probability of category and user preference: p(cat,user) = p(cat|user)*p(user)
    /// We can ommit p(user), because it is the same for each category in classiffication.
    /// </summary>
    public class NaiveBayesian
    {
        /// <summary>
        /// Current version
        /// </summary>
        int Version = 103;

        /// <summary>
        /// Feature managers' unique id.
        /// </summary>
        Guid FeatureSerializationId { get; set; }

        /// <summary>
        /// Training features
        /// </summary>
        FeatureManager _ftm;

        /// <summary>
        /// Conditional probability of a particular feature belongs to category
        /// </summary>
        Dictionary<string, Dictionary<int, double>> _featureLikellihood;

        /// <summary>
        /// Sum_ForEach_Feature( Count of number of Vouchers per feature)
        /// </summary>
        Dictionary<string, int> _featuresCount;

        /// <summary>
        /// number of vouchers per user + number of categories known by user
        /// </summary>
        Dictionary<int, int> _userCategCounts;

        /// <summary>
        /// Conditional probability of user choosing particular category
        /// </summary>
        Dictionary<int, Dictionary<string, double>> _categUserLikellihood;

        /// <summary>
        /// Category prior probability
        /// </summary>
        Dictionary<string, double> _categPrior;

        /// <summary>
        /// For performance reasons, precalculate for each category posteriors when all features is missing in test voucher
        /// So, it is a multiplication of negative conditional probabilities (we using Bernulli):
        /// posterior(c) = (1-p(ccategX | feature1))*....*(1-p(ccategX | featureN)))
        /// Because we use Log likellihood, the probabilities above is just a sum of 
        /// posterior(c) = log(1-p(ccategX | feature1))+....+log(1-p(ccategX | featureN)))
        /// Then when we predict, for each existing feature in the test voucher simply do:
        /// posterior(c) = posterior(c) - log(1-p(ccategX | featureY)) + log(p(ccategX | featureY))
        /// </summary>
        Dictionary<string, double> _posterior;

        /// <summary>
        /// Stores vouchers for training. We add new vouchers to existing collection when doing
        /// incremental training
        /// </summary>
        public List<VoucherWeightedFeatures> TrainingData { get; private set; }

        /// <summary>
        /// Load model from a file
        /// </summary>
        /// <param name="file"></param>
        /// <param name="ftm"></param>
        public void Load(string file, FeatureManager ftm)
        {
            _ftm = ftm;
            FeatureSerializationId = _ftm.SerializationId;

            using (FileStream fs = File.OpenRead(file))
            using (BinaryReader reader = new BinaryReader(fs))
            {
                Read(reader);
            }
        }

        public void Save(string file)
        {
            using (FileStream fs = File.OpenWrite(file))
            using (BinaryWriter writer = new BinaryWriter(fs))
            {
                Write(writer);
            }
        }

        /// <summary>
        /// Train model for selecting categories based on features
        /// </summary>
        /// <param name="ftm"></param>
        public void Train(FeatureManager ftm, IEnumerable<Voucher> vouchers)
        {
            _ftm = ftm;
            FeatureSerializationId = _ftm.SerializationId;

            TrainFeatures(vouchers);
            TrainUserPreference(vouchers);
        }

        /// <summary>
        /// Predict a set of possible categories the test voucher belongs to.
        /// </summary>
        /// <param name="ocrFeatures"></param>
        /// <param name="organizationId"></param>
        /// <param name="top"></param>
        /// <returns></returns>
        public List<Prediction> Predict(List<string> ocrFeatures, int organizationId, int top = 3)
        {
            var features = _ftm.ReadWeightedFeatures(ocrFeatures).ToDictionary(x => x.Item1, x => 0);

            Dictionary<string, int> selectedFeatures = _posterior.ToDictionary(x => x.Key, x => 0);
            Dictionary<string, double> posterior = _posterior.ToDictionary(x => x.Key, x => x.Value);

            foreach (var category in _featureLikellihood)
            {
                // set priori p(categ|user)
                // if we don't know the user we set p(categ)
                if (_categUserLikellihood.ContainsKey(organizationId))
                {
                    if (_categUserLikellihood[organizationId].ContainsKey(category.Key))
                    {
                        posterior[category.Key] += Math.Log(_categUserLikellihood[organizationId][category.Key]);
                    }
                    else
                    {
                        var unknownLikellihood = 1.0 / (_userCategCounts[organizationId] + 1);
                        posterior[category.Key] += Math.Log(unknownLikellihood);
                    }
                }
                else
                {
                    posterior[category.Key] += Math.Log(_categPrior[category.Key]);
                }

                foreach (var feature in features)
                {
                    double likellihood;
                    if (category.Value.ContainsKey(feature.Key))
                    {
                        // we know the feature
                        likellihood = category.Value[feature.Key];
                        selectedFeatures[category.Key]++;
                    }
                    else
                    {
                        // feature is not used in category
                        // therefore apply unknown feature likellihood
                        likellihood = 1.0 / (_featuresCount[category.Key] + _ftm.NumberOfValidFeatures + 1);
                    }
                    posterior[category.Key] += Math.Log(likellihood) - Math.Log(1 - likellihood);
                }
            }

            var tmp = posterior.Where(x => selectedFeatures[x.Key] > 0).Select(x => new Tuple<string, double>(x.Key, x.Value)).OrderByDescending(x => x.Item2).Take(top * 2).ToList();
            return SelectPreferred(tmp, top);
        }

        /// <summary>
        /// Makes ZScore distribution of predicted results and filter highest predictions
        /// The idea is that bigger the variance in the prediction, the higher chance is that the top scored
        /// results will be right
        /// </summary>
        /// <param name="values"></param>
        /// <param name="top"></param>
        /// <returns></returns>
        private List<Prediction> SelectPreferred(IEnumerable<Tuple<string, double>> values, int top)
        {
            Func<double, PredictionConfidence> getConfidence = (zscore) =>
            {
                if (zscore >= 1.0)
                {
                    return PredictionConfidence.High;
                }
                else if (zscore >= 0)
                {
                    return PredictionConfidence.Medium;
                }
                else
                {
                    return PredictionConfidence.Low;
                }
            };

            double avg = values.Select(x => x.Item2).Sum() / values.Count();
            double sd = Math.Sqrt(values.Sum(x => Math.Pow(x.Item2 - avg, 2)) / values.Count());

            return sd > 0 ? values
                .Select(x => new Prediction { Tag = x.Item1, ConfidenceLevel = getConfidence((x.Item2 - avg) / sd), Score = (x.Item2 - avg) / sd })
                .Where(x => x.ConfidenceLevel == PredictionConfidence.High || x.ConfidenceLevel == PredictionConfidence.Medium)
                .Take(top)
                .ToList()
                :
                // when no variance exists, we are a bit in a dilemma. It might be a few categories that precisely describes the test voucher, or, just a garbage result.
                (values.Count() <= top) ? values.Select(x => new Prediction { Tag = x.Item1, ConfidenceLevel = PredictionConfidence.Medium, Score = x.Item2 }).Take(top).ToList() : new List<Prediction>();
        }

        /// <summary>
        /// Train posteriors: p(Categ), p(User,Categ)
        /// </summary>
        /// <param name="vouchers"></param>
        void TrainUserPreference(IEnumerable<Voucher> vouchers)
        {
            // Compute conditional probabilities (likellihood) for Category given User:
            // user per category likellihood: p(Categ, User) = p(Categ|User) * p(User)
            // p(user) - can be ignored in prediction, since prediction is done per user

            // category probabilities
            _categPrior = vouchers.GroupBy(x => x.TagName).ToDictionary(x => x.Key, x => ((double)x.Count()) / vouchers.Count());

            // number of vouchers per user ( aka count vouchers per category ) plus number of categories
            _userCategCounts = vouchers
                .GroupBy(g => g.OrganizationId)
                .ToDictionary(x => x.Key, x => x.Count() + x.GroupBy(y => y.TagName).Count());

            // userCategLikellihood = Count number of Category Voucher per User +1 / Sum_Foreach_Category(Count number of Category Voucher per User + 1)
            _categUserLikellihood = vouchers.GroupBy(g => g.OrganizationId)
                .ToDictionary(x => x.Key, x =>
                    x.GroupBy(y => y.TagName)
                    .ToDictionary(y => y.Key, y => ((double)y.Count() + 1) / _userCategCounts[x.Key]));
        }

        /// <summary>
        /// Train model for selecting categories based on features
        /// </summary>
        /// <param name="ftm"></param>
        void TrainFeatures(IEnumerable<Voucher> vouchers)
        {
            if (TrainingData != null)
            {
                var voucherDict = TrainingData.GroupBy(x=>x.Voucher.Id).ToDictionary(x => x.Key, x=>0);
                TrainingData.AddRange(_ftm.ReadVouchersFeatures(vouchers.Where(x => !voucherDict.ContainsKey(x.Id))));
            }
            else
            {
                TrainingData = _ftm.ReadVouchersFeatures(vouchers);
            }

            //Compute conditional probabilities (likellihood) for Feature given Category:
            //p(F1|Categ) = Count of number of Vouchers per feature F1 + 1 /  Sum_ForEach_Feature( Count of number of Vouchers per feature Fx + 1)
            //  (.. + 1) - it is smoothing to account for missing features per category
            var categToFeaturesCounts = TrainingData.GroupBy(g => g.Voucher.TagName)
                .ToDictionary(x => x.Key, x =>
                    x.SelectMany(y => y.WeightedFeatures)
                        .GroupBy(y => y.Item1)
                        .ToDictionary(z => z.Key, z => z.Count()));

            // create a dictionary of features count per category
            // this is this part: Sum_ForEach_Feature( Count of number of Vouchers per feature)
            _featuresCount = categToFeaturesCounts.ToDictionary(x => x.Key, x => x.Value.Sum(y => y.Value));

            // p(Fx|Categ)
            _featureLikellihood = categToFeaturesCounts
                .ToDictionary(x => x.Key, x => x.Value
                    .ToDictionary(y => y.Key, y => ((double)y.Value + 1) / (_featuresCount[x.Key] + _ftm.NumberOfValidFeatures)));


            // Precompute posterior for each category:
            // as described in definition, this is just a performance optimization:
            // first we precalculate "aka" posteriors per category and during prediction
            // alter posterior with test document features and add user category likellihood
            _posterior = _featureLikellihood.ToDictionary(x => x.Key, x => 0.0);
            foreach (var category in _featureLikellihood)
            {
                foreach (var featureId in _ftm.ReadFeatures())
                {
                    double likellihood;

                    if (category.Value.ContainsKey(featureId))
                    {
                        // we know the feature
                        likellihood = category.Value[featureId];
                    }
                    else
                    {
                        // feature is not used in category
                        // therefore apply unknown feature likellihood
                        likellihood = 1.0 / (_featuresCount[category.Key] + _ftm.NumberOfValidFeatures + 1);
                    }
                    _posterior[category.Key] += Math.Log(1.0 - likellihood);
                }
            }
        }

        #region serialize

        public void Read(BinaryReader binaryReader)
        {
            Func<BinaryReader, int> deserialize_int = (reader) =>
            {
                return reader.ReadInt32();
            };

            Func<BinaryReader, double> deserialize_double = (reader) =>
            {
                return reader.ReadDouble();
            };

            Func<BinaryReader, string> deserialize_string = (reader) =>
            {
                return reader.ReadString();
            };

            Func<BinaryReader, Dictionary<int, double>> deserialize_dictionary_int_double = (reader) =>
            {
                Dictionary<int, double> ret = new Dictionary<int, double>();
                ReadFile<int, double>(reader, ret, deserialize_int, deserialize_double);
                return ret;
            };

            Func<BinaryReader, Dictionary<string, double>> deserialize_dictionary_string_double = (reader) =>
            {
                Dictionary<string, double> ret = new Dictionary<string, double>();
                ReadFile<string, double>(reader, ret, deserialize_string, deserialize_double);
                return ret;
            };

            Func<BinaryReader, Guid> deserialize_guid = (reader) =>
            {
                return Guid.Parse(reader.ReadString());
            };

            Func<BinaryReader, List<VoucherWeightedFeatures>> deserialize_list_VoucherWeightedFeatures = (reader) =>
            {
                List<VoucherWeightedFeatures> ret = new List<VoucherWeightedFeatures>();

                int cnt = reader.ReadInt32();

                for (int i = 0; i < cnt; i++)
                {
                    var voucher = new Voucher
                    {
                        TagName = reader.ReadString(),
                        Id = reader.ReadInt32(),
                        OrganizationId = reader.ReadInt32(),
                        Company = reader.ReadString(),
                        OcrPath = reader.ReadString()
                    };

                    // read OCR features
                    int nOcr = reader.ReadInt32();
                    voucher.OcrFeatures = new List<string>();
                    for (int y = 0; y < nOcr; y++)
                    {
                        voucher.OcrFeatures.Add(reader.ReadString());
                    }

                    var features = new List<Tuple<int, double>>();
                    int fcnt = reader.ReadInt32();
                    for (int y = 0; y < fcnt; y++)
                    {
                        features.Add(new Tuple<int, double>(reader.ReadInt32(), reader.ReadDouble()));
                    }

                    ret.Add(new VoucherWeightedFeatures { Voucher = voucher, WeightedFeatures = features });
                }

                return ret;
            };

            int version = deserialize_int(binaryReader);
            if (Version != version)
            {
                throw new InvalidOperationException("Version of Model file is not compatible");
            }

            FeatureSerializationId = deserialize_guid(binaryReader);
            if (FeatureSerializationId != _ftm.SerializationId)
            {
                throw new InvalidOperationException("Version of Feature Manager file is not compatible with Model file.");
            }

            TrainingData = deserialize_list_VoucherWeightedFeatures(binaryReader);

            _userCategCounts = new Dictionary<int, int>();
            ReadFile<int, int>(binaryReader, _userCategCounts, deserialize_int, deserialize_int);

            _featureLikellihood = new Dictionary<string, Dictionary<int, double>>();
            ReadFile<string, Dictionary<int, double>>(binaryReader, _featureLikellihood, deserialize_string, deserialize_dictionary_int_double);

            _categUserLikellihood = new Dictionary<int, Dictionary<string, double>>();
            ReadFile<int, Dictionary<string, double>>(binaryReader, _categUserLikellihood, deserialize_int, deserialize_dictionary_string_double);

            _categPrior = new Dictionary<string, double>();
            ReadFile<string, double>(binaryReader, _categPrior, deserialize_string, deserialize_double);

            _posterior = new Dictionary<string, double>();
            ReadFile<string, double>(binaryReader, _posterior, deserialize_string, deserialize_double);

            _featuresCount = new Dictionary<string, int>();
            ReadFile<string, int>(binaryReader, _featuresCount, deserialize_string, deserialize_int);
        }

        static void ReadFile<TKey, TValue>(BinaryReader reader, Dictionary<TKey, TValue> dict, Func<BinaryReader, TKey> deserializeKey, Func<BinaryReader, TValue> deserializeValue)
        {
            int count = reader.ReadInt32();

            for (int i = 0; i < count; i++)
            {
                TKey key = deserializeKey(reader);
                TValue val = deserializeValue(reader);
                dict.Add(key, val);
            }
        }

        public void Write(BinaryWriter binaryWriter)
        {
            Action<int, BinaryWriter> serialize_int = (Int, writer) =>
            {
                writer.Write(Int);
            };

            Action<double, BinaryWriter> serialize_double = (dbl, writer) =>
            {
                writer.Write(dbl);
            };

            Action<string, BinaryWriter> serialize_string = (str, writer) =>
            {
                writer.Write(str);
            };

            Action<Dictionary<string, double>, BinaryWriter> serialize_dictionary_string_double = (str, writer) =>
            {
                WriteFile<string, double>(writer, str, serialize_string, serialize_double);
            };

            Action<Dictionary<int, double>, BinaryWriter> serialize_dictionary_int_double = (str, writer) =>
            {
                WriteFile<int, double>(writer, str, serialize_int, serialize_double);
            };

            Action<Guid, BinaryWriter> serialize_guid = (guid, writer) =>
            {
                writer.Write(guid.ToString());
            };

            Action<List<VoucherWeightedFeatures>, BinaryWriter> serialize_list_VoucherWeightedFeatures = (voucherFeatures, writer) =>
            {
                writer.Write(voucherFeatures.Count);

                foreach (var voucherFeature in voucherFeatures)
                {
                    var voucher = voucherFeature.Voucher;

                    // write attributes
                    writer.Write(voucher.TagName);
                    writer.Write(voucher.Id);
                    writer.Write(voucher.OrganizationId);
                    writer.Write(voucher.Company);
                    writer.Write(voucher.OcrPath);

                    // write OCR features
                    writer.Write(voucher.OcrFeatures.Count);
                    foreach (var f in voucher.OcrFeatures)
                    {
                        writer.Write(f);
                    }

                    var features = voucherFeature.WeightedFeatures;
                    writer.Write(features.Count);
                    foreach (var feature in features)
                    {
                        writer.Write(feature.Item1);
                        writer.Write(feature.Item2);
                    }
                }
            };

            serialize_int(Version, binaryWriter);
            serialize_guid(FeatureSerializationId, binaryWriter);
            serialize_list_VoucherWeightedFeatures(TrainingData, binaryWriter);
            WriteFile<int, int>(binaryWriter, _userCategCounts, serialize_int, serialize_int);
            WriteFile<string, Dictionary<int, double>>(binaryWriter, _featureLikellihood, serialize_string, serialize_dictionary_int_double);
            WriteFile<int, Dictionary<string, double>>(binaryWriter, _categUserLikellihood, serialize_int, serialize_dictionary_string_double);
            WriteFile<string, double>(binaryWriter, _categPrior, serialize_string, serialize_double);
            WriteFile<string, double>(binaryWriter, _posterior, serialize_string, serialize_double);
            WriteFile<string, int>(binaryWriter, _featuresCount, serialize_string, serialize_int);
        }

        static void WriteFile<TKey, TValue>(BinaryWriter writer, Dictionary<TKey, TValue> dict, Action<TKey, BinaryWriter> serializeKey, Action<TValue, BinaryWriter> serializeValue)
        {
            // Put count.
            writer.Write(dict.Count);
            // Write pairs.
            foreach (var pair in dict)
            {
                serializeKey(pair.Key, writer);
                serializeValue(pair.Value, writer);
            }
        }

        #endregion
    }
}
